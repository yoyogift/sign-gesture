{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Upload and Run the Below Code in Google Collab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ukkcrwtDy8j"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IccAL7pYQ7ow"
      },
      "source": [
        "# **Import the Required Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSoCRT5fEIbg"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import gesture_recognizer\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YtW1ZJmRFwo"
      },
      "source": [
        "# **Add the Downloaded dataset to your google drives `My Drive` Section. Then use Below Code to connect it to this Project**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k3lYqiiEX0J"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy2oQmFTEdQQ"
      },
      "outputs": [],
      "source": [
        "my_folder_path = '/content/drive/MyDrive/sign_language_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwOAO6cWEfG6"
      },
      "outputs": [],
      "source": [
        "print(my_folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpknN9A7EgXa"
      },
      "outputs": [],
      "source": [
        "print(my_folder_path)\n",
        "labels = []\n",
        "for i in os.listdir(my_folder_path):\n",
        "  if os.path.isdir(os.path.join(my_folder_path, i)):\n",
        "    labels.append(i)\n",
        "for label in labels:\n",
        "  print(label + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1-uWZezEiTC"
      },
      "outputs": [],
      "source": [
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVykJGpmRjC6"
      },
      "source": [
        "# **From Below Run All the Steps to Train and then Download the Trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k8cJuBrEmKq",
        "outputId": "1ddb965e-d10b-4366-bac6-fc1be3f175af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-fbc998dd906b>:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n"
          ]
        }
      ],
      "source": [
        "NUM_EXAMPLES = 10\n",
        "\n",
        "for label in labels:\n",
        "  label_dir = os.path.join(my_folder_path, label)\n",
        "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
        "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n",
        "  for i in range(NUM_EXAMPLES):\n",
        "    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
        "    axs[i].get_xaxis().set_visible(False)\n",
        "    axs[i].get_yaxis().set_visible(False)\n",
        "  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2U5smNZEojS",
        "outputId": "e036b550-e900-4fd2-cf17-18fe1b08c963"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://storage.googleapis.com/mediapipe-assets/palm_detection_full.tflite to /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n",
            "Downloading https://storage.googleapis.com/mediapipe-assets/hand_landmark_full.tflite to /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n",
            "Downloading https://storage.googleapis.com/mediapipe-assets/gesture_embedder.tar.gz to /tmp/model_maker/gesture_recognizer/gesture_embedder\n"
          ]
        }
      ],
      "source": [
        "data = gesture_recognizer.Dataset.from_folder(\n",
        "    dirname=my_folder_path,\n",
        "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
        ")\n",
        "train_data, rest_data = data.split(0.8)\n",
        "validation_data, test_data = rest_data.split(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UWdONtUFzvn",
        "outputId": "2d3b3090-0a0f-4316-9da5-5d1af48644b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " hand_embedding (InputLayer)  [(None, 128)]            0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 128)              512       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 128)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " custom_gesture_recognizer_o  (None, 42)               5418      \n",
            " ut (Dense)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,930\n",
            "Trainable params: 5,674\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "743/743 [==============================] - 6s 6ms/step - loss: 2.6524 - categorical_accuracy: 0.2564 - val_loss: 0.9206 - val_categorical_accuracy: 0.6559 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "743/743 [==============================] - 8s 11ms/step - loss: 1.5697 - categorical_accuracy: 0.4926 - val_loss: 0.5648 - val_categorical_accuracy: 0.7473 - lr: 9.9000e-04\n",
            "Epoch 3/10\n",
            "743/743 [==============================] - 4s 6ms/step - loss: 1.2409 - categorical_accuracy: 0.5841 - val_loss: 0.4541 - val_categorical_accuracy: 0.7742 - lr: 9.8010e-04\n",
            "Epoch 4/10\n",
            "743/743 [==============================] - 6s 8ms/step - loss: 1.0796 - categorical_accuracy: 0.6131 - val_loss: 0.3885 - val_categorical_accuracy: 0.7849 - lr: 9.7030e-04\n",
            "Epoch 5/10\n",
            "743/743 [==============================] - 4s 6ms/step - loss: 0.9601 - categorical_accuracy: 0.6575 - val_loss: 0.3614 - val_categorical_accuracy: 0.8011 - lr: 9.6060e-04\n",
            "Epoch 6/10\n",
            "743/743 [==============================] - 4s 6ms/step - loss: 0.8950 - categorical_accuracy: 0.6837 - val_loss: 0.3305 - val_categorical_accuracy: 0.8226 - lr: 9.5099e-04\n",
            "Epoch 7/10\n",
            "743/743 [==============================] - 8s 10ms/step - loss: 0.8465 - categorical_accuracy: 0.6891 - val_loss: 0.3290 - val_categorical_accuracy: 0.8226 - lr: 9.4148e-04\n",
            "Epoch 8/10\n",
            "743/743 [==============================] - 4s 5ms/step - loss: 0.8020 - categorical_accuracy: 0.7039 - val_loss: 0.3184 - val_categorical_accuracy: 0.8333 - lr: 9.3207e-04\n",
            "Epoch 9/10\n",
            "743/743 [==============================] - 4s 5ms/step - loss: 0.7534 - categorical_accuracy: 0.7281 - val_loss: 0.2951 - val_categorical_accuracy: 0.8387 - lr: 9.2274e-04\n",
            "Epoch 10/10\n",
            "743/743 [==============================] - 8s 10ms/step - loss: 0.7393 - categorical_accuracy: 0.7147 - val_loss: 0.3044 - val_categorical_accuracy: 0.8441 - lr: 9.1352e-04\n"
          ]
        }
      ],
      "source": [
        "hparams = gesture_recognizer.HParams(export_dir=\"exported_model\")\n",
        "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrmP-4HXHdBm",
        "outputId": "c1ac4341-99b4-4c53-964e-d12b3ab29359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "186/186 [==============================] - 4s 4ms/step - loss: 0.3927 - categorical_accuracy: 0.8333\n",
            "Test loss:0.3927353620529175, Test accuracy:0.8333333134651184\n"
          ]
        }
      ],
      "source": [
        "loss, acc = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gbolt9PZH43d",
        "outputId": "7ce61174-bbd1-4d52-c8cb-e6abdb563b92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://storage.googleapis.com/mediapipe-assets/gesture_embedder.tflite to /tmp/model_maker/gesture_recognizer/gesture_embedder.tflite\n",
            "Downloading https://storage.googleapis.com/mediapipe-assets/canned_gesture_classifier.tflite to /tmp/model_maker/gesture_recognizer/canned_gesture_classifier.tflite\n",
            "best_model_weights.data-00000-of-00001\tepoch_models\t\t metadata.json\n",
            "best_model_weights.index\t\tgesture_recognizer.task\n",
            "checkpoint\t\t\t\tlogs\n"
          ]
        }
      ],
      "source": [
        "model.export_model()\n",
        "!ls exported_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGtmcbUWH71M",
        "outputId": "2d41508a-32e5-45d8-f543-5992fbca3655"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_ddbb167b-1c22-46bc-8f79-fb58d891d9d8\", \"gesture_recognizer.task\", 8480581)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "files.download('exported_model/gesture_recognizer.task')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "8930996e9b4848f840558511426186bf858d74b70f37fea4e5b9b386643acfbc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
